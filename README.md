# CVPR ID 5842 

## Python implementation of "Defending against adversarial attacks by randomized diversification"

### Test

The adversarial examples were generate by the attack propodsed by 
> Carlini Nicholas and Wagner David:  
> [Towards evaluating the robustness of neural networks](https://arxiv.org/pdf/1608.04644.pdf) 

The corresponding python implementation is taken from https://github.com/carlini/nn_robust_attacks

The attcked data are available at https://drive.google.com/file/d/1__1XOZN8zDIfm0O4HExavx9eUoPwMKmQ/view?usp=sharing






